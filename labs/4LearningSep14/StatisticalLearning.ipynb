{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "contained-pleasure",
   "metadata": {},
   "source": [
    "# Predictive modeling\n",
    "\n",
    "Let's start with the Crowdflower Airline Sentiment Twitter dataset. \n",
    "\n",
    "    A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\").\n",
    "    \n",
    "This dataset was produced by human categorization. But if you were working for an airline, you can easily imagine that you might want to assess customer sentiment (and reasons for negative sentiment) week by week without manually categorizing thousands of tweets. To do that, you might need to train some kind of predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surprised-sound",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from pathlib import Path\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-collapse",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "Let's start by loading the data and doing some exploratory analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "three-change",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14640, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>0</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment negativereason  retweet_count         airline  \\\n",
       "0           neutral            NaN              0  Virgin America   \n",
       "1          positive            NaN              0  Virgin America   \n",
       "2           neutral            NaN              0  Virgin America   \n",
       "3          negative     Bad Flight              0  Virgin America   \n",
       "4          negative     Can't Tell              0  Virgin America   \n",
       "\n",
       "                                                text  \n",
       "0                @VirginAmerica What @dhepburn said.  \n",
       "1  @VirginAmerica plus you've added commercials t...  \n",
       "2  @VirginAmerica I didn't today... Must mean I n...  \n",
       "3  @VirginAmerica it's really aggressive to blast...  \n",
       "4  @VirginAmerica and it's a really big bad thing...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetpath = Path('../../data/tweets/flight_sentiment.tsv')\n",
    "\n",
    "tweets = pd.read_csv(tweetpath, sep ='\\t')\n",
    "\n",
    "print(tweets.shape)\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-ozone",
   "metadata": {},
   "source": [
    "#### Exploratory data analysis\n",
    "\n",
    "It's always a good idea to inspect the data before modeling it.\n",
    "\n",
    "1. Find out what the top three reasons for unhappiness are. \n",
    "\n",
    "2. Visualize the distribution of positive, neutral, and negative ```airline_sentiment``` as a bar graph.\n",
    "\n",
    "3. Examine the full texts of 5 tweets in the \"Bad Flight\" category, to see what that actually means.\n",
    "\n",
    "To make full texts a little easier to inspect I'm going to start by telling Pandas to display the full column width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "worth-header",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "processed-anger",
   "metadata": {},
   "source": [
    "### Now create a term-document matrix (wordcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "through-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-envelope",
   "metadata": {},
   "source": [
    "I'm going to start occasionally using an alternative way to refer to Pandas columns, which is simply\n",
    "\n",
    "    ```df.columnname```\n",
    "    instead of\n",
    "    ```df['columnname']\n",
    "\n",
    "This can occasionally be confusing. If you name a column something like \"shape\" for instance, you could be in trouble! But it's a lot quicker to type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "executive-belle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0016</th>\n",
       "      <th>00pm</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>05</th>\n",
       "      <th>05am</th>\n",
       "      <th>05pm</th>\n",
       "      <th>08</th>\n",
       "      <th>...</th>\n",
       "      <th>yr</th>\n",
       "      <th>yrs</th>\n",
       "      <th>yuma</th>\n",
       "      <th>yup</th>\n",
       "      <th>yvr</th>\n",
       "      <th>yyz</th>\n",
       "      <th>zero</th>\n",
       "      <th>zkatcher</th>\n",
       "      <th>zone</th>\n",
       "      <th>zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  0016  00pm  02  03  05  05am  05pm  08  ...  yr  yrs  yuma  yup  \\\n",
       "0   0    0     0     0   0   0   0     0     0   0  ...   0    0     0    0   \n",
       "1   0    0     0     0   0   0   0     0     0   0  ...   0    0     0    0   \n",
       "2   0    0     0     0   0   0   0     0     0   0  ...   0    0     0    0   \n",
       "3   0    0     0     0   0   0   0     0     0   0  ...   0    0     0    0   \n",
       "4   0    0     0     0   0   0   0     0     0   0  ...   0    0     0    0   \n",
       "\n",
       "   yvr  yyz  zero  zkatcher  zone  zurich  \n",
       "0    0    0     0         0     0       0  \n",
       "1    0    0     0         0     0       0  \n",
       "2    0    0     0         0     0       0  \n",
       "3    0    0     0         0     0       0  \n",
       "4    0    0     0         0     0       0  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_wordcounts = vectorizer.fit_transform(tweets.text)\n",
    "wordcounts = sparse_wordcounts.toarray()\n",
    "tweetwords = pd.DataFrame(wordcounts, columns = vectorizer.get_feature_names())\n",
    "tweetwords.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-plenty",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "If we really cared about maximizing accuracy it would probably be worthwhile to think hard about the strategy we're using to count words. For instance, how will the CountVectorizer handle a hashtag like \"#soreback\"? How might we ideally want to handle it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-destination",
   "metadata": {},
   "source": [
    "Let's use the technique we learned last time to select some of the words most likely to predict positive or negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "great-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words = tweetwords.loc[tweets.airline_sentiment == 'negative', :].sum(axis = 'rows')\n",
    "positive_words = tweetwords.loc[tweets.airline_sentiment == 'positive', :].sum(axis = 'rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "stainless-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dunnings(word, series1, series2):\n",
    "    observed = pd.DataFrame({'series1': [series1[word], sum(series1) - series1[word]],\n",
    "                          'series2': [series2[word], sum(series2) - series2[word]]},\n",
    "                        index = ['word', 'all_others'])\n",
    "    total_words = observed.to_numpy().sum()\n",
    "    observed['word_totals'] = observed.sum(axis = 1)\n",
    "    observed = observed.append(observed.sum(axis = 0).rename(index = 'group_totals'))\n",
    "    observed.iat[2,2] = 0\n",
    "    observed['word_totals'] = observed['word_totals'] / sum(observed['word_totals'])\n",
    "    observed.loc['group_totals', : ] = observed.loc['group_totals', : ] / sum(observed.loc['group_totals', : ])\n",
    "    expected = np.outer(observed['word_totals'][0:2], observed.loc['group_totals', : ][0:2])\n",
    "    expected = pd.DataFrame(expected, index = ['word', 'all_others'], columns = ['series1', 'series2'])\n",
    "    expected = expected * total_words\n",
    "    \n",
    "    G = 0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            O = observed.iat[i, j] + .000001\n",
    "            E = expected.iat[i, j] + .000001\n",
    "            G = G + O * math.log(O / E)\n",
    "    \n",
    "    if (observed.iat[0, 0] / sum(observed.iloc[0: 2, 0])) < (observed.iat[0, 1] / sum(observed.iloc[0 : 2, 1])):\n",
    "        G = -G    # we provide a signed version of the statistic to distinguish\n",
    "                  # overrepresentation in the two categories\n",
    "    \n",
    "    return 2 * G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "burning-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "dunningslist = []\n",
    "\n",
    "for w in vectorizer.get_feature_names():\n",
    "    G = get_dunnings(w, positive_words, negative_words)\n",
    "    dunningslist.append(G)\n",
    "\n",
    "dunnings = pd.Series(dunningslist, index = vectorizer.get_feature_names())\n",
    "dunnings = dunnings.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dramatic-market",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no          -166.815332\n",
       "hours       -165.829494\n",
       "hold        -154.872916\n",
       "not         -140.097004\n",
       "cancelled   -131.809294\n",
       "delayed      -86.589061\n",
       "flightled    -83.499564\n",
       "worst        -80.546255\n",
       "hour         -74.594059\n",
       "call         -74.179613\n",
       "why          -65.693835\n",
       "been         -59.965623\n",
       "is           -59.156369\n",
       "hrs          -57.239008\n",
       "usairways    -56.335208\n",
       "waiting      -54.147933\n",
       "can          -49.627402\n",
       "because      -48.000829\n",
       "on           -47.943896\n",
       "told         -47.675738\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dunnings[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-kennedy",
   "metadata": {},
   "source": [
    "No one mentions \"luggage\" or \"hours\" if they're happy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "extended-making",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wonderful          68.246391\n",
       "excellent          68.246391\n",
       "kudos              79.079912\n",
       "rock               82.611096\n",
       "appreciate        103.044759\n",
       "good              109.744104\n",
       "http              127.821593\n",
       "co                144.173384\n",
       "much              158.245714\n",
       "virginamerica     170.800522\n",
       "best              192.147680\n",
       "amazing           218.417748\n",
       "you               235.157336\n",
       "awesome           279.904867\n",
       "love              286.215863\n",
       "southwestair      316.750251\n",
       "jetblue           429.077287\n",
       "great             551.760163\n",
       "thanks           1214.224929\n",
       "thank            1280.943710\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dunnings[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-break",
   "metadata": {},
   "source": [
    "Let's select a list of 400 highly predictive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "regional-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "extremefeatures = list(dunnings.index[0:100].values) + list(dunnings.index[-100: ].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-class",
   "metadata": {},
   "source": [
    "### A first model: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-denmark",
   "metadata": {},
   "source": [
    "To keep this simple to start with, let's create a dataframe with even numbers of positive and negative tweets. Let's say 200 positive ones and 200 negative ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "danish-assessment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 5000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_tweets = tweetwords.loc[tweets.airline_sentiment == 'positive', :]\n",
    "negative_tweets = tweetwords.loc[tweets.airline_sentiment == 'negative', :]\n",
    "\n",
    "sample400_allwords = pd.concat([positive_tweets.iloc[0:200, : ], negative_tweets.iloc[0: 200, : ]], axis = 'rows')\n",
    "sample400_allwords.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-accused",
   "metadata": {},
   "source": [
    "Let's also create a version of this data limited to our list of 200 highly predictive (extreme) features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "postal-scotland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample400_ext200 = sample400_allwords.loc[ : , extremefeatures]  # why does this work?\n",
    "sample400_ext200.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-third",
   "metadata": {},
   "source": [
    "Now we need a list of true labels for the model to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "exempt-mayor",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_sentiment_400 = [1] * 200 + [0] * 200\n",
    "# what will that produce?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-manhattan",
   "metadata": {},
   "source": [
    "Now let's train a first model. We'll use just the 200 words that Dunnings identified as extreme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "opponent-arbor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8825"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_bayes = MultinomialNB(alpha = 0.1)   # Alpha sets the \"smoothing.\" Notice we're not using much.\n",
    "sm_bayes.fit(sample400_ext200, true_sentiment_400)   # Here we actually fit the model\n",
    "\n",
    "sm_predictions = sm_bayes.predict(sample400_ext200)   # Now let's see what the model predicts if we give it\n",
    "sum(sm_predictions == true_sentiment_400) / len(sm_predictions)  # the same data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-mississippi",
   "metadata": {},
   "source": [
    "Not a bad prediction. But how would this model do if we gave it new data?\n",
    "\n",
    "Let's make a bigger dataset, with the next 1500 negative tweets and the next 1500 positive tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "advisory-mother",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 5000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next3000 =  pd.concat([positive_tweets.iloc[200: 1700, : ], negative_tweets.iloc[200: 1700, : ]], axis = 'rows')\n",
    "true_sentiment_3000 = [1] * 1500 + [0] * 1500\n",
    "next3000.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-sewing",
   "metadata": {},
   "source": [
    "Now a version limited to 200 words that are strongly associated with positive or negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "laughing-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "next3000_ext200 = next3000.loc[ : , extremefeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "herbal-queen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7953333333333333"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_predictions = sm_bayes.predict(next3000_ext200)\n",
    "sum(sm_predictions == true_sentiment_3000) / len(sm_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-armenia",
   "metadata": {},
   "source": [
    "Oh, that's much worse. Maybe we could improve this by increasing the number of words we use to predict? Let's train on the version of our 400-tweet dataset that has all 5000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "accurate-trance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.995"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_bayes = MultinomialNB(alpha = 0.1)\n",
    "big_bayes.fit(sample400_allwords, true_sentiment_400)\n",
    "big_predictions = big_bayes.predict(sample400_allwords)\n",
    "sum(big_predictions == true_sentiment_400) / len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-object",
   "metadata": {},
   "source": [
    "Ah, yes. Including lots more words makes our model almost perfectly accurate on the 400 tweets we used to train it.\n",
    "\n",
    "Maybe that will also improve accuracy on the new data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "everyday-centre",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7343333333333333"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_predictions = big_bayes.predict(next3000.loc[ : , : ])\n",
    "sum(big_predictions == true_sentiment_3000) / len(big_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-secretariat",
   "metadata": {},
   "source": [
    "Yikes!! This is not helping.\n",
    "\n",
    "#### Discussion:\n",
    "\n",
    "What do we need to do to improve accuracy?\n",
    "\n",
    "#### Exercise:\n",
    "\n",
    "Working in small groups, construct a loop that implements the solution we've described, and finds the parameter setting that maximizes accuracy *on our large 3000-tweet dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-combining",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fitted-arena",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "\n",
    "Separating test and train datasets is a strategy to avoid overfitting, and the exercise above was designed to demonstrate why overfitting is a danger.\n",
    "\n",
    "But there are several aspects of what we did above that aren't actually recommended practice.\n",
    "\n",
    "1. First, we had a small training dataset (400) and a big test set (3000). But you normally have a training dataset that is much larger than the test dataset. Ratios of 7 to 3 or 9 to 1 are common.\n",
    "\n",
    "2. It's not a good idea to do *feature selection* on the whole dataset. We selected our \"extreme 400\" features by calculating Dunning's log-likelihood on the whole corpus of tweets--but really, if we wanted to use a feature selection strategy, we should only calculate Dunning's on the *training* dataset. Otherwise we may get misleadingly high accuracy. Can you figure out why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-politics",
   "metadata": {},
   "source": [
    "### Inspect probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "vocational-messenger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24716759, 0.75283241],\n",
       "       [0.66905453, 0.33094547],\n",
       "       [0.49552751, 0.50447249],\n",
       "       [0.08209022, 0.91790978],\n",
       "       [0.1042556 , 0.8957444 ],\n",
       "       [0.48221875, 0.51778125],\n",
       "       [0.01739582, 0.98260418],\n",
       "       [0.05254278, 0.94745722],\n",
       "       [0.24080718, 0.75919282],\n",
       "       [0.32471968, 0.67528032]])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_probabilities[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "removable-basic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_predictions[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "dying-johnson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fca117e3990>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(true_sentiment_400 + np.random.random(size = 400)/3, [x[1] for x in sm_probabilities],\n",
    "            c = (sm_predictions == true_sentiment_400), cmap = 'coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-replica",
   "metadata": {},
   "source": [
    "## Logistic regression, and how we might really do this\n",
    "\n",
    "Naive Bayes is simple, which makes it a good example to teach first. But in real-life problems (especially if documents are reasonably long), regularized logistic regression is more commonly where a modeling process will start. There are lots of approaches more sophisticated than logistic regression, but LR makes a good \"baseline,\" and is often in practice good enough for the task.\n",
    "\n",
    "As we work through a solution here, I'm going to dispense with some of the simplifying assumptions we initially made. For instance, we constructed an imaginary world where classes of tweets were \"balanced\" (of equal size). As we know, that's not actually true in our real-world data. \n",
    "\n",
    "So let's go back to our original complete doc-term matrix, ```tweetwords```.\n",
    "\n",
    "#### preparing the data\n",
    "\n",
    "Because this matrix is based on raw word counts, it conflates word frequency with tweet length. Tweet-length might be an interesting predictive feature, but we probably don't want it mixed into all the different words. Let's factor it out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "informed-keeping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     35\n",
       "1     72\n",
       "2     71\n",
       "3    126\n",
       "4     55\n",
       "5    135\n",
       "6     79\n",
       "7    108\n",
       "8     47\n",
       "9     80\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetlengths = tweets['text'].str.len()\n",
    "tweetlengths[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "southeast-tennessee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0016</th>\n",
       "      <th>00pm</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>05</th>\n",
       "      <th>05am</th>\n",
       "      <th>05pm</th>\n",
       "      <th>08</th>\n",
       "      <th>...</th>\n",
       "      <th>yrs</th>\n",
       "      <th>yuma</th>\n",
       "      <th>yup</th>\n",
       "      <th>yvr</th>\n",
       "      <th>yyz</th>\n",
       "      <th>zero</th>\n",
       "      <th>zkatcher</th>\n",
       "      <th>zone</th>\n",
       "      <th>zurich</th>\n",
       "      <th>#tweetlen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14640 rows Ã— 5001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        00  000  0016  00pm   02   03   05  05am  05pm   08  ...  yrs  yuma  \\\n",
       "0      0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0  0.0  ...  0.0   0.0   \n",
       "1      0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0  0.0  ...  0.0   0.0   \n",
       "2      0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0  0.0  ...  0.0   0.0   \n",
       "3      0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0  0.0  ...  0.0   0.0   \n",
       "4      0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0  0.0  ...  0.0   0.0   \n",
       "...    ...  ...   ...   ...  ...  ...  ...   ...   ...  ...  ...  ...   ...   \n",
       "14635  0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0  0.0  ...  0.0   0.0   \n",
       "14636  0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0  0.0  ...  0.0   0.0   \n",
       "14637  0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0  0.0  ...  0.0   0.0   \n",
       "14638  0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0  0.0  ...  0.0   0.0   \n",
       "14639  0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0  0.0  ...  0.0   0.0   \n",
       "\n",
       "       yup  yvr  yyz  zero  zkatcher  zone  zurich  #tweetlen  \n",
       "0      0.0  0.0  0.0   0.0       0.0   0.0     0.0         35  \n",
       "1      0.0  0.0  0.0   0.0       0.0   0.0     0.0         72  \n",
       "2      0.0  0.0  0.0   0.0       0.0   0.0     0.0         71  \n",
       "3      0.0  0.0  0.0   0.0       0.0   0.0     0.0        126  \n",
       "4      0.0  0.0  0.0   0.0       0.0   0.0     0.0         55  \n",
       "...    ...  ...  ...   ...       ...   ...     ...        ...  \n",
       "14635  0.0  0.0  0.0   0.0       0.0   0.0     0.0         63  \n",
       "14636  0.0  0.0  0.0   0.0       0.0   0.0     0.0        150  \n",
       "14637  0.0  0.0  0.0   0.0       0.0   0.0     0.0         60  \n",
       "14638  0.0  0.0  0.0   0.0       0.0   0.0     0.0        135  \n",
       "14639  0.0  0.0  0.0   0.0       0.0   0.0     0.0        138  \n",
       "\n",
       "[14640 rows x 5001 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordfreqs = tweetwords.divide(tweetlengths, axis = 'rows')\n",
    "wordfreqs['#tweetlen'] = tweetlengths\n",
    "wordfreqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-duncan",
   "metadata": {},
   "source": [
    "We're going to want to hold out a \"test\" set to finally confirm the quality of our model. This should be randomly selected from all tweets, and we don't know the current sequence is random. So let's shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "trained-transformation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0016</th>\n",
       "      <th>00pm</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>05</th>\n",
       "      <th>05am</th>\n",
       "      <th>05pm</th>\n",
       "      <th>08</th>\n",
       "      <th>...</th>\n",
       "      <th>yrs</th>\n",
       "      <th>yuma</th>\n",
       "      <th>yup</th>\n",
       "      <th>yvr</th>\n",
       "      <th>yyz</th>\n",
       "      <th>zero</th>\n",
       "      <th>zkatcher</th>\n",
       "      <th>zone</th>\n",
       "      <th>zurich</th>\n",
       "      <th>#tweetlen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2414</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6518</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13944</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        00  000  0016  00pm   02   03   05  05am  05pm   08  ...  yrs  yuma  \\\n",
       "1369   0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0  0.0  ...  0.0   0.0   \n",
       "2414   0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0  0.0  ...  0.0   0.0   \n",
       "6518   0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0  0.0  ...  0.0   0.0   \n",
       "478    0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0  0.0  ...  0.0   0.0   \n",
       "13944  0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0  0.0  ...  0.0   0.0   \n",
       "\n",
       "       yup  yvr  yyz  zero  zkatcher  zone  zurich  #tweetlen  \n",
       "1369   0.0  0.0  0.0   0.0       0.0   0.0     0.0        139  \n",
       "2414   0.0  0.0  0.0   0.0       0.0   0.0     0.0        128  \n",
       "6518   0.0  0.0  0.0   0.0       0.0   0.0     0.0        138  \n",
       "478    0.0  0.0  0.0   0.0       0.0   0.0     0.0         83  \n",
       "13944  0.0  0.0  0.0   0.0       0.0   0.0     0.0         49  \n",
       "\n",
       "[5 rows x 5001 columns]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordfreqs = wordfreqs.sample(frac = 1)\n",
    "wordfreqs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-remains",
   "metadata": {},
   "source": [
    "Notice that the indexes were shuffled too. This allows us to create a reordered ```tweets``` frame in the same order. We're going to need this if we want to know the true sentiment for all those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "ranging-qatar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>negative</td>\n",
       "      <td>Late Flight</td>\n",
       "      <td>0</td>\n",
       "      <td>United</td>\n",
       "      <td>@united Missed UA1568 connection due 2 mechanical failure UA1543 (\"uplink prob\"). I will lose ~$400 in clients Mon. How can you compensate?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2414</th>\n",
       "      <td>negative</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>0</td>\n",
       "      <td>United</td>\n",
       "      <td>@united But it's doubtful he'll fly United again and he will be traveling a lot as a sales manager covering all of North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6518</th>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>@SouthwestAir You guys must have been swamped, even took our corporate travel a long time to get in, but everything worked out in the end.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica how come ABC is the only one of the network channels you don't have?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13944</th>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir Dallas/Fort Worth flight number 5320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      airline_sentiment negativereason  retweet_count         airline  \\\n",
       "1369           negative    Late Flight              0          United   \n",
       "2414           negative     Can't Tell              0          United   \n",
       "6518            neutral            NaN              0       Southwest   \n",
       "478             neutral            NaN              0  Virgin America   \n",
       "13944           neutral            NaN              0        American   \n",
       "\n",
       "                                                                                                                                              text  \n",
       "1369   @united Missed UA1568 connection due 2 mechanical failure UA1543 (\"uplink prob\"). I will lose ~$400 in clients Mon. How can you compensate?  \n",
       "2414              @united But it's doubtful he'll fly United again and he will be traveling a lot as a sales manager covering all of North America  \n",
       "6518    @SouthwestAir You guys must have been swamped, even took our corporate travel a long time to get in, but everything worked out in the end.  \n",
       "478                                                            @VirginAmerica how come ABC is the only one of the network channels you don't have?  \n",
       "13944                                                                                            @AmericanAir Dallas/Fort Worth flight number 5320  "
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reorderedtweets = tweets.loc[wordfreqs.index, : ]\n",
    "reorderedtweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-parish",
   "metadata": {},
   "source": [
    "For our simplifying purposes let's get rid of 'neutral's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "smoking-league",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11541, 5001)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordfreqs = wordfreqs.loc[reorderedtweets['airline_sentiment'] != 'neutral', : ]\n",
    "reorderedtweets = reorderedtweets.loc[reorderedtweets['airline_sentiment'] != 'neutral', : ]\n",
    "wordfreqs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-henry",
   "metadata": {},
   "source": [
    "Now let's separate the tweets into test and train sets. There's not a hard and fast rule on proportions here; it depends on how much data you have. People with tons of data may do a 90-10 split. 11,541 rows is not exactly tons and we want a good readout of final accuracy, so let's set 2000 aside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "polish-preparation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1369     0\n",
       "2414     0\n",
       "11698    0\n",
       "4509     0\n",
       "3745     0\n",
       "6889     0\n",
       "1501     0\n",
       "10572    0\n",
       "3837     0\n",
       "5895     0\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testfreqs = wordfreqs.iloc[0: 2000, : ]\n",
    "test_y = (reorderedtweets['airline_sentiment'][0: 2000] == 'positive').astype(int)  # try taking the last part out\n",
    "test_y[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "recognized-mileage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3783     0\n",
       "2467     0\n",
       "4864     1\n",
       "10872    0\n",
       "13816    0\n",
       "1774     0\n",
       "13936    0\n",
       "1049     0\n",
       "9661     0\n",
       "2890     1\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainfreqs = wordfreqs.iloc[2000 : , : ]\n",
    "train_y = (reorderedtweets['airline_sentiment'][2000: ] == 'positive').astype(int) \n",
    "train_y[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-rates",
   "metadata": {},
   "source": [
    "We want regularization to spread evenly across different variables, even though some variables (like ```#tweetlen```) are numerically much bigger than others. So we typically \"scale\" our X matrices before using logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "scheduled-aerospace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0016</th>\n",
       "      <th>00pm</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>05</th>\n",
       "      <th>05am</th>\n",
       "      <th>05pm</th>\n",
       "      <th>08</th>\n",
       "      <th>...</th>\n",
       "      <th>yrs</th>\n",
       "      <th>yuma</th>\n",
       "      <th>yup</th>\n",
       "      <th>yvr</th>\n",
       "      <th>yyz</th>\n",
       "      <th>zero</th>\n",
       "      <th>zkatcher</th>\n",
       "      <th>zone</th>\n",
       "      <th>zurich</th>\n",
       "      <th>#tweetlen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.036303</td>\n",
       "      <td>-0.044143</td>\n",
       "      <td>-0.010238</td>\n",
       "      <td>-0.020373</td>\n",
       "      <td>-0.02147</td>\n",
       "      <td>-0.014391</td>\n",
       "      <td>-0.024719</td>\n",
       "      <td>-0.014306</td>\n",
       "      <td>-0.017497</td>\n",
       "      <td>-0.016838</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021645</td>\n",
       "      <td>-0.01671</td>\n",
       "      <td>-0.017593</td>\n",
       "      <td>-0.02016</td>\n",
       "      <td>-0.035746</td>\n",
       "      <td>-0.053645</td>\n",
       "      <td>-0.017555</td>\n",
       "      <td>-0.020478</td>\n",
       "      <td>-0.017619</td>\n",
       "      <td>-0.297827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.036303</td>\n",
       "      <td>-0.044143</td>\n",
       "      <td>-0.010238</td>\n",
       "      <td>-0.020373</td>\n",
       "      <td>-0.02147</td>\n",
       "      <td>-0.014391</td>\n",
       "      <td>-0.024719</td>\n",
       "      <td>-0.014306</td>\n",
       "      <td>-0.017497</td>\n",
       "      <td>-0.016838</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021645</td>\n",
       "      <td>-0.01671</td>\n",
       "      <td>-0.017593</td>\n",
       "      <td>-0.02016</td>\n",
       "      <td>-0.035746</td>\n",
       "      <td>-0.053645</td>\n",
       "      <td>-0.017555</td>\n",
       "      <td>-0.020478</td>\n",
       "      <td>-0.017619</td>\n",
       "      <td>0.854251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.036303</td>\n",
       "      <td>-0.044143</td>\n",
       "      <td>-0.010238</td>\n",
       "      <td>-0.020373</td>\n",
       "      <td>-0.02147</td>\n",
       "      <td>-0.014391</td>\n",
       "      <td>-0.024719</td>\n",
       "      <td>-0.014306</td>\n",
       "      <td>-0.017497</td>\n",
       "      <td>-0.016838</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021645</td>\n",
       "      <td>-0.01671</td>\n",
       "      <td>-0.017593</td>\n",
       "      <td>-0.02016</td>\n",
       "      <td>-0.035746</td>\n",
       "      <td>-0.053645</td>\n",
       "      <td>-0.017555</td>\n",
       "      <td>-0.020478</td>\n",
       "      <td>-0.017619</td>\n",
       "      <td>-0.787461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.036303</td>\n",
       "      <td>-0.044143</td>\n",
       "      <td>-0.010238</td>\n",
       "      <td>-0.020373</td>\n",
       "      <td>-0.02147</td>\n",
       "      <td>-0.014391</td>\n",
       "      <td>-0.024719</td>\n",
       "      <td>-0.014306</td>\n",
       "      <td>-0.017497</td>\n",
       "      <td>-0.016838</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021645</td>\n",
       "      <td>-0.01671</td>\n",
       "      <td>-0.017593</td>\n",
       "      <td>-0.02016</td>\n",
       "      <td>-0.035746</td>\n",
       "      <td>-0.053645</td>\n",
       "      <td>-0.017555</td>\n",
       "      <td>-0.020478</td>\n",
       "      <td>-0.017619</td>\n",
       "      <td>-1.824331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.036303</td>\n",
       "      <td>-0.044143</td>\n",
       "      <td>-0.010238</td>\n",
       "      <td>-0.020373</td>\n",
       "      <td>-0.02147</td>\n",
       "      <td>-0.014391</td>\n",
       "      <td>-0.024719</td>\n",
       "      <td>-0.014306</td>\n",
       "      <td>-0.017497</td>\n",
       "      <td>-0.016838</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021645</td>\n",
       "      <td>-0.01671</td>\n",
       "      <td>-0.017593</td>\n",
       "      <td>-0.02016</td>\n",
       "      <td>-0.035746</td>\n",
       "      <td>-0.053645</td>\n",
       "      <td>-0.017555</td>\n",
       "      <td>-0.020478</td>\n",
       "      <td>-0.017619</td>\n",
       "      <td>0.911855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         00       000      0016      00pm       02        03        05  \\\n",
       "0 -0.036303 -0.044143 -0.010238 -0.020373 -0.02147 -0.014391 -0.024719   \n",
       "1 -0.036303 -0.044143 -0.010238 -0.020373 -0.02147 -0.014391 -0.024719   \n",
       "2 -0.036303 -0.044143 -0.010238 -0.020373 -0.02147 -0.014391 -0.024719   \n",
       "3 -0.036303 -0.044143 -0.010238 -0.020373 -0.02147 -0.014391 -0.024719   \n",
       "4 -0.036303 -0.044143 -0.010238 -0.020373 -0.02147 -0.014391 -0.024719   \n",
       "\n",
       "       05am      05pm        08  ...       yrs     yuma       yup      yvr  \\\n",
       "0 -0.014306 -0.017497 -0.016838  ... -0.021645 -0.01671 -0.017593 -0.02016   \n",
       "1 -0.014306 -0.017497 -0.016838  ... -0.021645 -0.01671 -0.017593 -0.02016   \n",
       "2 -0.014306 -0.017497 -0.016838  ... -0.021645 -0.01671 -0.017593 -0.02016   \n",
       "3 -0.014306 -0.017497 -0.016838  ... -0.021645 -0.01671 -0.017593 -0.02016   \n",
       "4 -0.014306 -0.017497 -0.016838  ... -0.021645 -0.01671 -0.017593 -0.02016   \n",
       "\n",
       "        yyz      zero  zkatcher      zone    zurich  #tweetlen  \n",
       "0 -0.035746 -0.053645 -0.017555 -0.020478 -0.017619  -0.297827  \n",
       "1 -0.035746 -0.053645 -0.017555 -0.020478 -0.017619   0.854251  \n",
       "2 -0.035746 -0.053645 -0.017555 -0.020478 -0.017619  -0.787461  \n",
       "3 -0.035746 -0.053645 -0.017555 -0.020478 -0.017619  -1.824331  \n",
       "4 -0.035746 -0.053645 -0.017555 -0.020478 -0.017619   0.911855  \n",
       "\n",
       "[5 rows x 5001 columns]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainscaler = StandardScaler()\n",
    "trainXscaled = trainscaler.fit_transform(trainfreqs)\n",
    "trainXscaled = pd.DataFrame(trainXscaled, columns = trainfreqs.columns)\n",
    "trainXscaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "formed-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "testscaler = StandardScaler()\n",
    "testXscaled = testscaler.fit_transform(testfreqs)\n",
    "testXscaled = pd.DataFrame(testXscaled, columns = testfreqs.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-pattern",
   "metadata": {},
   "source": [
    "**Discussion**: why are we scaling these two matrices separately? Why not scale the whole thing together before dividing into test and train sets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-startup",
   "metadata": {},
   "source": [
    "#### cross-validating a model\n",
    "\n",
    "If we just wanted to train and test a single model--and if we only wanted to measure \"accuracy\"--this would be simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "patient-portsmouth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9045"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logist = LogisticRegression(C = .1, max_iter = 1000) \n",
    "logist.fit(trainXscaled, train_y)\n",
    "\n",
    "predictions = logist.predict(testXscaled)\n",
    "sum(predictions == test_y) / len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-stake",
   "metadata": {},
   "source": [
    "Why is this not a very safe measurement? Well, how many positive tweets do we actually have in our test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "wrapped-point",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.204"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test_y) / len(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-charles",
   "metadata": {},
   "source": [
    "So you can get 80% \"accuracy\" just by predicting \"negative\" in every case.\n",
    "\n",
    "A better measurement is F1 score:\n",
    "\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "The simple way to calculate this is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "enormous-engine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7573062261753495"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-martin",
   "metadata": {},
   "source": [
    "but that doesn't help us understand the number, so ...\n",
    "\n",
    "#### EXERCISE 2:\n",
    "\n",
    "working in small groups, calculate precision: the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. \n",
    "\n",
    "then recall: the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. \n",
    "\n",
    "Then calculate your f1 score, which should equal what *you* got above. Note that since we randomized the test/train split, everyone's result will be slightly different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-favor",
   "metadata": {},
   "source": [
    "### handling imbalanced classes\n",
    "\n",
    "To keep your model from always choosing \"negative,\" you can weight the classes inversely to their frequency. This will make errors in rare classes more costly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "aging-source",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7627118644067796"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logist = LogisticRegression(C = .1, max_iter = 1000, class_weight = 'balanced') \n",
    "logist.fit(trainXscaled, train_y)\n",
    "\n",
    "predictions = logist.predict(testXscaled)\n",
    "f1_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-press",
   "metadata": {},
   "source": [
    "The C parameter sets smoothing. It's like alpha in Naive Bayes, except size is reversed here: low numbers equal stronger smoothing. We could fiddle around with C until we got a good number on our test set, but then we might be overfitting our test set. \n",
    "\n",
    "**Discussion:** Why?\n",
    "\n",
    "An alternative is to fiddle around with the training set until we get a good C parameter, and then test that number on the test set (being careful not to look at it until the final stage.)\n",
    "\n",
    "But we don't want to overfit the training set in the process of \"fiddling around\" on it. Otherwise we'll tend to choose less regularization than we really need for the test set.\n",
    "\n",
    "A solution here is to \"cross-validate\" on the training set: repeatedly divide it, train on say 4/5th of the data, and test on the remaining fifth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "instant-somerset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([4.55598092, 5.27096176, 4.72088623, 5.31768298, 4.35501695]), 'score_time': array([0.04901886, 0.04940629, 0.04912186, 0.04850292, 0.05249   ]), 'test_score': array([0.72796935, 0.69879518, 0.72435105, 0.71890547, 0.72609819])}\n",
      "\n",
      "Mean f1: 0.7192238487826067\n"
     ]
    }
   ],
   "source": [
    "logist = LogisticRegression(C = .01, max_iter = 1000, class_weight = 'balanced')\n",
    "results = cross_validate(logist, trainXscaled, train_y, cv = 5, scoring = 'f1')\n",
    "print(results)\n",
    "print()\n",
    "print('Mean f1:', np.mean(results['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-retirement",
   "metadata": {},
   "source": [
    "Technically, to be extremely cautious, we would \"scale\" each fold of the cross-validation separately. This can be done in sklearn using a \"pipeline,\" but it adds more complexity than we need for this exercise and doesn't actually change our result greatly.\n",
    "\n",
    "#### EXERCISE 3:\n",
    "\n",
    "Construct a for-loop that tests C parameters ranging from .00001 to 10, varying by powers of 10.\n",
    "\n",
    "    [.00001, .0001, .001, .01, .1, 1, 10]\n",
    "\n",
    "In each case, cross-validate on trainXscaled and report the f1 score. When you've got the C parameter that works best, use that model to make a prediction on testXscaled, and calculate both f1 score and accuracy for that prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "exotic-talent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C parameter: 1e-05\n",
      "Mean f1: 0.7572185140239014\n",
      "C parameter: 0.0001\n",
      "Mean f1: 0.7663891450949031\n",
      "C parameter: 0.001\n",
      "Mean f1: 0.7725587587027671\n",
      "C parameter: 0.01\n",
      "Mean f1: 0.757108314347154\n",
      "C parameter: 0.1\n",
      "Mean f1: 0.7412962850808001\n",
      "C parameter: 1\n",
      "Mean f1: 0.7192238487826067\n",
      "C parameter: 10\n",
      "Mean f1: 0.707171536076827\n"
     ]
    }
   ],
   "source": [
    "for c_param in [.00001, .0001, .001, .01, .1, 1, 10]:\n",
    "    logist = LogisticRegression(C = c_param, max_iter = 1000, class_weight = 'balanced') \n",
    "    results = cross_validate(logist, trainXscaled, train_y, cv = 5, scoring = 'f1')\n",
    "    print('C parameter:', c_param)\n",
    "    print('Mean f1:', np.mean(results['test_score']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "secret-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-destiny",
   "metadata": {},
   "source": [
    "#### identify predictive features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "authorized-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "logist = LogisticRegression(C = c_param, max_iter = 1000, class_weight = 'balanced')\n",
    "logist.fit(trainXscaled, train_y)\n",
    "coefficients = [x for x in zip(logist.coef_[0], vectorizer.get_feature_names())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "attended-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "american-arthur",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.14645392410087024, 'not'),\n",
       " (-0.1065035301088209, 'no'),\n",
       " (-0.10599519278651948, 'delayed'),\n",
       " (-0.10488388696004632, 'hours'),\n",
       " (-0.0938868300555908, 'worst'),\n",
       " (-0.0905628326048827, 'hold'),\n",
       " (-0.0874277694338639, 'cancelled'),\n",
       " (-0.07963021967663711, 'is'),\n",
       " (-0.07477410536535237, 'why'),\n",
       " (-0.07308465929735496, 'flightled'),\n",
       " (-0.07287158188138247, 'hour'),\n",
       " (-0.06628520332193458, 'late'),\n",
       " (-0.06520203434410853, 'because'),\n",
       " (-0.0643067621413151, 'what'),\n",
       " (-0.06390583004076458, 'delay'),\n",
       " (-0.06383261920722895, 'how'),\n",
       " (-0.06324038697974534, 'can'),\n",
       " (-0.06294424437475006, 'never'),\n",
       " (-0.06254919159789342, 'need'),\n",
       " (-0.061676302196237646, 'call')]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "narrow-adventure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.06057929144441297, 'much'),\n",
       " (0.0620508864285614, 'got'),\n",
       " (0.06797044743599927, 'glad'),\n",
       " (0.07000721675405233, 'job'),\n",
       " (0.07292900011222268, 'worries'),\n",
       " (0.0772813526006825, 'kudos'),\n",
       " (0.08352568275232157, 'excellent'),\n",
       " (0.08354652254183786, 'you'),\n",
       " (0.09121280322945946, 'virginamerica'),\n",
       " (0.09230862065832485, 'appreciate'),\n",
       " (0.09492070250563944, 'good'),\n",
       " (0.10664744501790183, 'jetblue'),\n",
       " (0.11174476176562308, 'southwestair'),\n",
       " (0.12098072865659446, 'amazing'),\n",
       " (0.12464225851181865, 'best'),\n",
       " (0.12597304718784247, 'love'),\n",
       " (0.13473076947581752, 'awesome'),\n",
       " (0.19367423590775812, 'great'),\n",
       " (0.26218025055389255, 'thank'),\n",
       " (0.2999161560822741, 'thanks')]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-freeze",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
