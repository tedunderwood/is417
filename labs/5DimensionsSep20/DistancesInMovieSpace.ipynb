{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incredible-running",
   "metadata": {},
   "source": [
    "# Thinking about movies and words in high-dimensional space\n",
    "\n",
    "As chapter 6 of Jurafsky and Martin explained, both the columns and the rows of a term-document matrix can be understood geometrically as points in space. (Or, if you prefer, as *vectors* drawn from the origin of a coordinate system to a particular point.)\n",
    "\n",
    "![A term-doc matrix and vector representation of same](jurafskyvector.png)\n",
    "\n",
    "To make visualization easier, the textbook has chosen two dimensions here. But the space defined by the term-document matrix really has four dimensions. We can't easily draw a picture of four-dimensional space, but it's perfectly possible to measure distance there.\n",
    "\n",
    "See section 6.4 of Jurafsky and Martin for a mathematical explanation of \"cosine similarity.\" The cosine ranges from -1 to 1, and is largest when two vectors are very *similar.* (As you can see below, the cosine gets smaller as the angle X gets larger.)\n",
    "\n",
    "![https://www.quora.com/Mathematics-Why-does-the-obtuse-angle-of-cos-x-give-a-negative-value](cosine.png)\n",
    "\n",
    "To convert it into something more like a distance, it's conventional to measure\n",
    "\n",
    "1 - cos(X)\n",
    "\n",
    "which can be called \"cosine distance.\"\n",
    "\n",
    "In this notebook, we'll explore ways to reason geometrically about cultural objects by treating them as points in high-dimensional space. The examples we'll use are movies and movie characters, but much of this logic will apply also to images.\n",
    "\n",
    "We will\n",
    "\n",
    "1. Compare features (words) and instances (movies) by measuring the cosine distance between them,\n",
    "\n",
    "2. Cluster movie scripts in semantic space, and\n",
    "\n",
    "3. Flatten high-dimensional space into a plane, using a variety of *dimension reduction* techniques. Some information will be lost when we do this, but it's still a useful way to explore complex data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-questionnaire",
   "metadata": {},
   "source": [
    "### Import useful modules and read data\n",
    "\n",
    "We'll be using the same dialogue data we used in HW3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "union-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import math, re, random\n",
    "from scipy.spatial.distance import cosine\n",
    "from collections import Counter\n",
    "from ast import literal_eval\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-richards",
   "metadata": {},
   "source": [
    "We start by reading in the familiar dataframe, with one row per character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stuffed-reasoning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mid</th>\n",
       "      <th>cid</th>\n",
       "      <th>cname</th>\n",
       "      <th>mname</th>\n",
       "      <th>gender</th>\n",
       "      <th>wordcount</th>\n",
       "      <th>year</th>\n",
       "      <th>genres</th>\n",
       "      <th>comedy</th>\n",
       "      <th>thriller</th>\n",
       "      <th>drama</th>\n",
       "      <th>romance</th>\n",
       "      <th>lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m0</td>\n",
       "      <td>u0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>f</td>\n",
       "      <td>959</td>\n",
       "      <td>1999</td>\n",
       "      <td>['comedy', 'romance']</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>They do not! / I hope so. / Let's go. / Okay -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m0</td>\n",
       "      <td>u2</td>\n",
       "      <td>CAMERON</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>m</td>\n",
       "      <td>527</td>\n",
       "      <td>1999</td>\n",
       "      <td>['comedy', 'romance']</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>They do to! / She okay? / Wow / No / The \"real...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m0</td>\n",
       "      <td>u4</td>\n",
       "      <td>JOEY</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>m</td>\n",
       "      <td>278</td>\n",
       "      <td>1999</td>\n",
       "      <td>['comedy', 'romance']</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Listen, I want to talk to you about the prom. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m0</td>\n",
       "      <td>u5</td>\n",
       "      <td>KAT</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>f</td>\n",
       "      <td>1217</td>\n",
       "      <td>1999</td>\n",
       "      <td>['comedy', 'romance']</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Perm? / It's just you. / What? To completely d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m0</td>\n",
       "      <td>u6</td>\n",
       "      <td>MANDELLA</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>f</td>\n",
       "      <td>157</td>\n",
       "      <td>1999</td>\n",
       "      <td>['comedy', 'romance']</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>William - he asked me to meet him here. / Have...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mid cid     cname                       mname gender  wordcount  year  \\\n",
       "0  m0  u0    BIANCA  10 things i hate about you      f        959  1999   \n",
       "1  m0  u2   CAMERON  10 things i hate about you      m        527  1999   \n",
       "2  m0  u4      JOEY  10 things i hate about you      m        278  1999   \n",
       "3  m0  u5       KAT  10 things i hate about you      f       1217  1999   \n",
       "4  m0  u6  MANDELLA  10 things i hate about you      f        157  1999   \n",
       "\n",
       "                  genres  comedy  thriller  drama  romance  \\\n",
       "0  ['comedy', 'romance']    True     False  False     True   \n",
       "1  ['comedy', 'romance']    True     False  False     True   \n",
       "2  ['comedy', 'romance']    True     False  False     True   \n",
       "3  ['comedy', 'romance']    True     False  False     True   \n",
       "4  ['comedy', 'romance']    True     False  False     True   \n",
       "\n",
       "                                               lines  \n",
       "0  They do not! / I hope so. / Let's go. / Okay -...  \n",
       "1  They do to! / She okay? / Wow / No / The \"real...  \n",
       "2  Listen, I want to talk to you about the prom. ...  \n",
       "3  Perm? / It's just you. / What? To completely d...  \n",
       "4  William - he asked me to meet him here. / Have...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogpath = Path('../../data/movie_dialogue.tsv')\n",
    "\n",
    "chars = pd.read_csv(dialogpath, sep = '\\t')\n",
    "\n",
    "chars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-foundation",
   "metadata": {},
   "source": [
    "### Aggregate dialogue at the movie level\n",
    "\n",
    "But for this problem, it's more manageable (at least initially) to talk about movies rather than characters. So let's aggregate the dialogue at the movie level.\n",
    "\n",
    "```.groupby()``` helps with this, but it's still not a totally straightforward task. It's hard to avoid writing a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "comic-crack",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mid</th>\n",
       "      <th>wordcount</th>\n",
       "      <th>year</th>\n",
       "      <th>genres</th>\n",
       "      <th>comedy</th>\n",
       "      <th>thriller</th>\n",
       "      <th>drama</th>\n",
       "      <th>romance</th>\n",
       "      <th>lines</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mname</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10 things i hate about you</th>\n",
       "      <td>m0</td>\n",
       "      <td>5159</td>\n",
       "      <td>1999</td>\n",
       "      <td>['comedy', 'romance']</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>They do not! / I hope so. / Let's go. / Okay -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492: conquest of paradise</th>\n",
       "      <td>m1</td>\n",
       "      <td>1474</td>\n",
       "      <td>1992</td>\n",
       "      <td>['adventure', 'biography', 'drama', 'history']</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Asia can be found to the west -- and I will pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affliction</th>\n",
       "      <td>m10</td>\n",
       "      <td>5896</td>\n",
       "      <td>1997</td>\n",
       "      <td>['drama', 'mystery', 'thriller']</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>All the figures show is that Gordon LaRiviere ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>innerspace</th>\n",
       "      <td>m100</td>\n",
       "      <td>5181</td>\n",
       "      <td>1987</td>\n",
       "      <td>['action', 'adventure', 'comedy', 'crime', 'sc...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Sending what we know back to U.S.T. via satell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the insider</th>\n",
       "      <td>m101</td>\n",
       "      <td>6418</td>\n",
       "      <td>1999</td>\n",
       "      <td>['biography', 'drama', 'thriller']</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>The New York Times ran a blow by blow of what ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             mid  wordcount  year  \\\n",
       "mname                                               \n",
       "10 things i hate about you    m0       5159  1999   \n",
       "1492: conquest of paradise    m1       1474  1992   \n",
       "affliction                   m10       5896  1997   \n",
       "innerspace                  m100       5181  1987   \n",
       "the insider                 m101       6418  1999   \n",
       "\n",
       "                                                                       genres  \\\n",
       "mname                                                                           \n",
       "10 things i hate about you                              ['comedy', 'romance']   \n",
       "1492: conquest of paradise     ['adventure', 'biography', 'drama', 'history']   \n",
       "affliction                                   ['drama', 'mystery', 'thriller']   \n",
       "innerspace                  ['action', 'adventure', 'comedy', 'crime', 'sc...   \n",
       "the insider                                ['biography', 'drama', 'thriller']   \n",
       "\n",
       "                            comedy  thriller  drama  romance  \\\n",
       "mname                                                          \n",
       "10 things i hate about you    True     False  False     True   \n",
       "1492: conquest of paradise   False     False   True    False   \n",
       "affliction                   False      True   True    False   \n",
       "innerspace                    True     False  False    False   \n",
       "the insider                  False      True   True    False   \n",
       "\n",
       "                                                                        lines  \n",
       "mname                                                                          \n",
       "10 things i hate about you  They do not! / I hope so. / Let's go. / Okay -...  \n",
       "1492: conquest of paradise  Asia can be found to the west -- and I will pr...  \n",
       "affliction                  All the figures show is that Gordon LaRiviere ...  \n",
       "innerspace                  Sending what we know back to U.S.T. via satell...  \n",
       "the insider                 The New York Times ran a blow by blow of what ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple-to-understand, if clunky, way to group by movie\n",
    "# We start by creating a dictionary with column labels as \n",
    "# the keys; each value will be a list that turns into a\n",
    "# column in a new dataframe with one row per movie.\n",
    "\n",
    "movie_dict = dict()\n",
    "exclude = {'cid', 'cname', 'gender'}  # movies don't have these\n",
    "\n",
    "for col in chars.columns:\n",
    "    if not col in exclude:\n",
    "        movie_dict[col] = []    # initialize the list\n",
    "\n",
    "for movie_id, df in chars.groupby('mid'):\n",
    "    \n",
    "    dialogue = ' '.join([x for x in df['lines']])  #\n",
    "    movie_dict['lines'].append(dialogue)\n",
    "    \n",
    "    for col in chars.columns:\n",
    "        if col in exclude or col == 'lines' or col == 'wordcount':\n",
    "            continue\n",
    "        else:\n",
    "            movie_dict[col].append(df[col].values[0]) # for most columns all rows\n",
    "                                                # of the grouped df are the same\n",
    "                                                # so just take the first entry\n",
    "    \n",
    "    movie_dict['wordcount'].append(np.sum(df['wordcount']))  # sum\n",
    "    \n",
    "movies = pd.DataFrame(movie_dict)\n",
    "movies = movies.set_index('mname')\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-music",
   "metadata": {},
   "source": [
    "#### Build vocabulary\n",
    "\n",
    "In the past we've let the CountVectorizer define its own vocabulary. But it's actually a little naive to create a vocabulary by just counting the number of words in a corpus. It's often better to select words by *document frequency*. If \"Leia,\" for instance, appears 105 times in 3 movies, the document frequency of Leia is 3--which might be a better reflection of its importance in the corpus than 105."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "technological-taste",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter()\n",
    "\n",
    "for script in movies['lines']:\n",
    "    words = re.split('\\W', script)\n",
    "    one_each = set([w for w in words if len(w) > 1])  # get rid of one-letter words, often noise\n",
    "    for w in one_each:\n",
    "        if not w.isdigit() and not w[0].isupper():    # get rid of numbers and names\n",
    "            vocab[w.lower()] += 1\n",
    "        \n",
    "vocab = vocab.most_common(5000)   # This is a Counter() method that returns paired\n",
    "                                  # keys and counts for the keys with highest counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-internet",
   "metadata": {},
   "source": [
    "Let's see what's actually in ```vocab.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "polish-photograph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words: [('the', 601), ('it', 600), ('you', 600), ('to', 600), ('what', 600), ('me', 599), ('of', 599), ('that', 599), ('know', 599), ('are', 599)] \n",
      "\n",
      "Least common words (in top 5000) [('rated', 12), ('referred', 12), ('document', 12), ('ratings', 12), ('canceled', 12), ('inject', 12), ('fragile', 12), ('worthwhile', 12), ('ankles', 12), ('rear', 12)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Most common words:\", vocab[0:10], '\\n')\n",
    "print(\"Least common words (in top 5000)\", vocab[-10: ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-devices",
   "metadata": {},
   "source": [
    "We want to split apart the words (we'll call them ```lexicon```) and the document frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cardiovascular-converter",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = [x[0] for x in vocab]     # this syntax is called a 'list comprehension'\n",
    "docfreqs = [x[1] for x in vocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-setting",
   "metadata": {},
   "source": [
    "Now count all the words in lexicon. Notice that the words are now in frequency order rather than alphabetical order. There will also be fewer numbers and names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-sphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(vocabulary = lexicon)\n",
    "all_movie_ids = movies.index.tolist()\n",
    "sparse_counts = vectorizer.fit_transform(movies['lines']) # the vectorizer produces something\n",
    "                                                               # called a 'sparse matrix'; we need to\n",
    "                                                               # unpack it\n",
    "moviecounts = pd.DataFrame(sparse_counts.toarray(), index = all_movie_ids, \n",
    "                            columns = vectorizer.get_feature_names())\n",
    "moviecounts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-lease",
   "metadata": {},
   "source": [
    "A final refinement. If we're looking for similarities and differences between films, a word like 'gambler' is more valuable than would be suggested by its rarity, and a word like 'the' is a little less useful than its frequency would suggest. \n",
    "\n",
    "One common way to address this is to adjust the term frequencies (columns) by dividing them by the document frequencies of those terms. This technique--which slightly increases the weight assigned to rare words--is called tf-idf (term-frequency-inverse-document-frequency). It was developed as a way of improving information retrieval, by more appropriately weighting the words in a search engine.\n",
    "\n",
    "We could do that here by saying\n",
    "\n",
    "    moviefreqs = moviecounts.divide(docfreqs, axis = 'columns') \n",
    "\n",
    "What works even better, however, is a technique we learned last time, when we were applying logistic regression to a high-dimensional matrix of word frequencies.\n",
    "\n",
    "First we turned raw word counts into probabilities, by dividing by total text length. (Summing rows.)\n",
    "\n",
    "Then we \"scaled\" those probabilities to give all words the same weight. (Technically, we subtracted the column mean from each column, and divided by column standard deviation.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "quantitative-antique",
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptlengths = moviecounts.sum(axis = 'columns')\n",
    "moviefreqs = moviecounts.divide(scriptlengths, axis = 'rows')  # turn counts into probabilities\n",
    "\n",
    "scaler = StandardScaler()\n",
    "movies_normed = scaler.fit_transform(moviefreqs)    # now scale the columns\n",
    "\n",
    "# now we add back the column and row labels that the Scaler removed\n",
    "movies_normed = pd.DataFrame(movies_normed, columns = moviecounts.columns, index = moviecounts.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-theater",
   "metadata": {},
   "source": [
    "The step where we divided by script length would not be absolutely necessary if we just wanted to use cosine distance to compare difference movies. Cosine distance measures the angle between vectors, not the length of the vectors. See [the figure I borrowed from Quora](https://www.quora.com/Mathematics-Why-does-the-obtuse-angle-of-cos-x-give-a-negative-value) below: the Eucidean distance between D and E is greater than the distance between A and B, because the vectors to D and E are longer. But the angle, and cosine distance, between both pairs of points is the same.\n",
    "\n",
    "![A figure from Quora](cosversuseuclid.png)\n",
    "\n",
    "So, as long as we're comparing movies by cosine, it doesn't matter if the numbers in some rows are bigger than in other rows--what matters for the angle are the relative *proportions* between columns. \n",
    "\n",
    "But what effect would this have if we flipped the matrix and decided to compare different words according to their angle in movie space?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-story",
   "metadata": {},
   "source": [
    "### Using cosine distance to group words and movies\n",
    "\n",
    "When we used a term-document matrix for predictive modeling, we needed to have a known set of labels--indicating the genre of a movie or the gender of a character. That's *supervised* modeling. But we can also use this data in an *unsupervised* fashion to detect patterns. There will be many kinds of unsupervised (or self-supervised) modeling, but a first step in that direction is to measure distances between rows or between columns. \n",
    "\n",
    "The code below allows you to enter a column (a word), and then finds other words that are closest to it in \"movie-space\" -- that is, in terms of their distribution across movies. Is closeness-in-movie-space the same thing as a word's \"meaning\"? Experiment, see what you think, and discuss in breakout groups. I recommend trying \"monster,\" \"cop,\" and \"gamble\" as well as a few common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "superior-orange",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "word to match?  star\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5118386773960988 entertain\n",
      "0.5518303303832697 emotion\n",
      "0.5540334537001183 comic\n",
      "0.620605263497499 arrives\n",
      "0.6206565724749187 data\n"
     ]
    }
   ],
   "source": [
    "wordtomatch = input('word to match? ')\n",
    "\n",
    "if wordtomatch not in movies_normed.columns:\n",
    "    print(\"I'm sorry, that's not a word in the columns of the matrix.\")\n",
    "else:\n",
    "    matches = []\n",
    "\n",
    "    for w in movies_normed.columns:\n",
    "        if w == wordtomatch:\n",
    "            continue\n",
    "        else:\n",
    "            cosdist = cosine(movies_normed[w], movies_normed[wordtomatch])\n",
    "            matches.append((cosdist, w))\n",
    "\n",
    "    matches.sort()\n",
    "    for distance, title in matches[0:5]:\n",
    "        print(distance, title)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-papua",
   "metadata": {},
   "source": [
    "#### EXERCISE 1\n",
    "\n",
    "We can use the same technique to compare rows (movie scripts) in word space. \n",
    "\n",
    "All you need to do to adapt the code above is to flip the dataframe movies_normed by using the ```.transpose()``` method of a Pandas dataframe. Start by saying \n",
    "\n",
    "    wordspace = movies_normed.transpose()\n",
    "\n",
    "Then inspect what you've got, and adapt the code above so that the user can enter a movie title, and get five movies that are close matches in \"word space.\"\n",
    "\n",
    "Do these results conform to our intuitions about the similarity of movies? Experiment and discuss. I suggest trying \"the godfather,\" \"jaws,\" \"shakespeare in love,\" and \"vertigo.\" What aspects of artistic similarity get ignored, or exaggerated, if we use this measure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "deluxe-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-antarctica",
   "metadata": {},
   "source": [
    "## Dimension-reduction: principal component analysis\n",
    "\n",
    "Comparing individual pairs of movies is fun. But we don't have to think about similarity in this one-by-one way.\n",
    "\n",
    "Suppose we want to get an overview of the structure of this dataset. Does \"genre\" strongly determine the dialogue in a movie or is release date a more powerful factor?\n",
    "\n",
    "Ideally, we'd like to inspect the relationships between scripts in thousand-dimensional space. Since our eyes don't see well in a thousand dimensions, we need a way to compress those similarities and differences down to two dimensions. But what two dimensions should we use? No single pair of words is likely to be informative.\n",
    "\n",
    "One classic way to reduce the dimensionality of a dataset is called \"principal component analysis,\" or PCA. PCA constructs new variables that are linear combinations of the variables you started with, and guarantees that these new \"components\" of variance are orthogonal (at right angles) to each other. The actual math it uses to do this is hard to explain if you're not familiar with matrix multiplication and factoring, but the goal of the process is easy to understand; the animations [here](https://setosa.io/ev/principal-component-analysis/) and [here](https://builtin.com/data-science/step-step-explanation-principal-component-analysis) may help.\n",
    "\n",
    "#### Selecting a subset of movies and scaling the matrix\n",
    "\n",
    "Let's start by selecting 320 movies at random. I'm setting the ```random_state``` to 12 so that--I hope!--we all get loosely similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "headed-calgary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>it</th>\n",
       "      <th>you</th>\n",
       "      <th>to</th>\n",
       "      <th>what</th>\n",
       "      <th>me</th>\n",
       "      <th>of</th>\n",
       "      <th>that</th>\n",
       "      <th>know</th>\n",
       "      <th>are</th>\n",
       "      <th>...</th>\n",
       "      <th>rated</th>\n",
       "      <th>referred</th>\n",
       "      <th>document</th>\n",
       "      <th>ratings</th>\n",
       "      <th>canceled</th>\n",
       "      <th>inject</th>\n",
       "      <th>fragile</th>\n",
       "      <th>worthwhile</th>\n",
       "      <th>ankles</th>\n",
       "      <th>rear</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mname</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>insomnia</th>\n",
       "      <td>48</td>\n",
       "      <td>28</td>\n",
       "      <td>52</td>\n",
       "      <td>57</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>38</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cast away</th>\n",
       "      <td>84</td>\n",
       "      <td>77</td>\n",
       "      <td>135</td>\n",
       "      <td>66</td>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "      <td>22</td>\n",
       "      <td>37</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the silence of the lambs</th>\n",
       "      <td>142</td>\n",
       "      <td>78</td>\n",
       "      <td>256</td>\n",
       "      <td>147</td>\n",
       "      <td>44</td>\n",
       "      <td>78</td>\n",
       "      <td>61</td>\n",
       "      <td>67</td>\n",
       "      <td>32</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a nightmare on elm street 4: the dream master</th>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the deer hunter</th>\n",
       "      <td>56</td>\n",
       "      <td>77</td>\n",
       "      <td>169</td>\n",
       "      <td>33</td>\n",
       "      <td>60</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>48</td>\n",
       "      <td>37</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               the  it  you   to  what  me  \\\n",
       "mname                                                                        \n",
       "insomnia                                        48  28   52   57    22  17   \n",
       "cast away                                       84  77  135   66    32  34   \n",
       "the silence of the lambs                       142  78  256  147    44  78   \n",
       "a nightmare on elm street 4: the dream master   12  25   25   16    10   7   \n",
       "the deer hunter                                 56  77  169   33    60   9   \n",
       "\n",
       "                                               of  that  know  are  ...  \\\n",
       "mname                                                               ...   \n",
       "insomnia                                       18    38    11    4  ...   \n",
       "cast away                                      22    37    17   12  ...   \n",
       "the silence of the lambs                       61    67    32   17  ...   \n",
       "a nightmare on elm street 4: the dream master   6     4     5    2  ...   \n",
       "the deer hunter                                18    48    37   20  ...   \n",
       "\n",
       "                                               rated  referred  document  \\\n",
       "mname                                                                      \n",
       "insomnia                                           0         0         0   \n",
       "cast away                                          0         0         0   \n",
       "the silence of the lambs                           0         0         0   \n",
       "a nightmare on elm street 4: the dream master      0         0         0   \n",
       "the deer hunter                                    0         0         0   \n",
       "\n",
       "                                               ratings  canceled  inject  \\\n",
       "mname                                                                      \n",
       "insomnia                                             0         0       0   \n",
       "cast away                                            0         0       0   \n",
       "the silence of the lambs                             0         0       0   \n",
       "a nightmare on elm street 4: the dream master        0         0       0   \n",
       "the deer hunter                                      0         0       0   \n",
       "\n",
       "                                               fragile  worthwhile  ankles  \\\n",
       "mname                                                                        \n",
       "insomnia                                             0           0       0   \n",
       "cast away                                            0           0       0   \n",
       "the silence of the lambs                             0           0       0   \n",
       "a nightmare on elm street 4: the dream master        0           0       0   \n",
       "the deer hunter                                      0           0       0   \n",
       "\n",
       "                                               rear  \n",
       "mname                                                \n",
       "insomnia                                          0  \n",
       "cast away                                         0  \n",
       "the silence of the lambs                          0  \n",
       "a nightmare on elm street 4: the dream master     0  \n",
       "the deer hunter                                   0  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moviesubset = movies.sample(320, random_state = 12)\n",
    "\n",
    "subsetcounts = moviecounts.loc[moviesubset.index, : ]\n",
    "subsetcounts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-easter",
   "metadata": {},
   "source": [
    "When we were just measuring cosine similarities between movies, it didn't matter how long the vectors were; we were just measuring angles. But PCA will create components by measuring the total amount of variance on different axes. The difference between long and short scripts would matter and distort our results if we didn't start by turning word counts into probabilities. To do that, we divide by total script length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "hindu-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptlengths = subsetcounts.sum(axis = 'columns')\n",
    "subset_freqs = subsetcounts.divide(scriptlengths, axis = 'rows')\n",
    "\n",
    "# I don't use this in the code below, but you could use it experimentally\n",
    "tfidf_subset = subset_freqs.divide(docfreqs, axis = 'columns') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-farmer",
   "metadata": {},
   "source": [
    "We will also usually get more informative results if we scale all the columns to have equal magnitude. This reduces the influence of rare words even more aggressively than the tf-idf scaling we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "strange-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "normed_subset = scaler.fit_transform(subset_freqs)\n",
    "normed_subset = pd.DataFrame(normed_subset, columns = subsetcounts.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-merit",
   "metadata": {},
   "source": [
    "#### Principal component analysis\n",
    "\n",
    "For an intuitive visual explainer on PCA, you can consult [a blog post by Victor Powell.](https://setosa.io/ev/principal-component-analysis/) For our purposes, you can envision it as a process that chooses the best \"camera angle\" to view a high-dimensional space -- an angle where the points spread out as much as possible in a smaller-dimensional representation, like say a plane.\n",
    "\n",
    "Because you're viewing the original space at an angle, the axes in the new reduced space each represent some *algebraic combination* of the original dimensions. So you can't label the x and y axes in a PCA plot in any simple way. All of the features (words) in your original dataset may contribute somehow to these abstract \"dimensions.\"\n",
    "\n",
    "As usual, scikit-learn makes the math easy. You just import PCA and specify that you want to reduce your data to two \"components\" (aka dimensions). Then you can fit the model and transform your data in a single step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "northern-peninsula",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "components = pca.fit_transform(normed_subset)  # what happens if you change this to tfidf_subset?\n",
    "components.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-generation",
   "metadata": {},
   "source": [
    "Here are our 320 movies spread out in a plane defined by the two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "incoming-groove",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fde58f933d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(components[ : , 0], components[ : , 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-belarus",
   "metadata": {},
   "source": [
    "#### Producing a more informative visualization\n",
    "\n",
    "So far, that's just a glob of dots. What can we learn from it?\n",
    "\n",
    "To start with, it would be helpful if we could color the dots by genre. We can't simply assign a color to each genre, because genres in this dataset are not exclusive. Each movie can have a number of different genre tags.\n",
    "\n",
    "But we could, for instance, visualize two genres at once, giving one color to genre A, one to genre B, and a third color to the films with both genres.\n",
    "\n",
    "By the way, what genres do we have in this subset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "loved-piano",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action',\n",
       " 'adventure',\n",
       " 'animation',\n",
       " 'biography',\n",
       " 'comedy',\n",
       " 'crime',\n",
       " 'documentary',\n",
       " 'drama',\n",
       " 'family',\n",
       " 'fantasy',\n",
       " 'film-noir',\n",
       " 'history',\n",
       " 'horror',\n",
       " 'music',\n",
       " 'musical',\n",
       " 'mystery',\n",
       " 'romance',\n",
       " 'sci-fi',\n",
       " 'short',\n",
       " 'sport',\n",
       " 'thriller',\n",
       " 'war',\n",
       " 'western'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genreset = set()\n",
    "\n",
    "for idx, row in moviesubset.iterrows():\n",
    "    genres = literal_eval(row.genres)\n",
    "    for g in genres:\n",
    "        genreset.add(g)\n",
    "        \n",
    "genreset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-creativity",
   "metadata": {},
   "source": [
    "Let's create a function that we can use to color the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "optical-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre2color(row, orangegenre, bluegenre):\n",
    "    color = 'other'\n",
    "    \n",
    "    genres = literal_eval(row['genres'])  # this turns the \"genres\" string into an actual Python list\n",
    "                        # e.g. \"['comedy', 'romance']\" becomes ['comedy', 'romance']\n",
    "    \n",
    "    genrecount = 0\n",
    "    \n",
    "    if orangegenre in genres:\n",
    "        color = orangegenre\n",
    "        genrecount += 1\n",
    "    if bluegenre in genres:\n",
    "        color = bluegenre\n",
    "        genrecount += 1\n",
    "        \n",
    "    if genrecount > 1:\n",
    "        color = 'both'\n",
    "    \n",
    "    return color\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-jewel",
   "metadata": {},
   "source": [
    "Coloring the dots is a good start on making this readable, but ... it would be a little nicer if we could label some of the films.\n",
    "\n",
    "Unfortunately, we have too many dots to label them all. It would be nice if we could select some widely spaced examples of the genres we're coloring.\n",
    "\n",
    "That could be done manually, but I've written the function below to do it automatically. You should basically ignore the details here, because all this function is doing is selecting a list of line numbers that correspond to widely spaced points. If you needed to do choose dots to label yourself, you could always do it manually; that would be much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afraid-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outliers(plotframe):\n",
    "    \n",
    "    # This is more complex than it really needs to be.\n",
    "    # I'm using a complex strategy to make sure the\n",
    "    # outliers we label are evenly distributed and not so\n",
    "    # close to each other that the labels overlap.\n",
    "    \n",
    "    outliers = []\n",
    "    skip = set()\n",
    "    \n",
    "    meanx = np.mean(plotframe['x'])\n",
    "    meany = np.mean(plotframe['y'])\n",
    "\n",
    "    for points_labeled in range(20):\n",
    "\n",
    "        distances = []\n",
    "\n",
    "        for linenum in range(plotframe.shape[0]):\n",
    "            if linenum in outliers or linenum in skip or plotframe.color[linenum] == 'other':\n",
    "                continue\n",
    "            else:\n",
    "                euclidnorm = math.sqrt((plotframe.x[linenum] - meanx) ** 2 + (plotframe.y[linenum] - meany) **2)\n",
    "                distances.append((euclidnorm, linenum))\n",
    "\n",
    "        distances.sort()\n",
    "\n",
    "        chosen = distances[-1][1]\n",
    "        tooclose = False\n",
    "\n",
    "        for already_have in outliers:\n",
    "            verticaldist = abs(plotframe.y[already_have] - plotframe.y[chosen])\n",
    "            if verticaldist < 0.015:\n",
    "                tooclose = True\n",
    "                skip.add(chosen)\n",
    "\n",
    "        if not tooclose:        \n",
    "            outliers.append(chosen)\n",
    "\n",
    "        meanx = np.mean([meanx, np.mean(plotframe.loc[outliers, 'x'])])\n",
    "        meany = np.mean([meany, np.mean(plotframe.loc[outliers, 'y'])])\n",
    "    \n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-albany",
   "metadata": {},
   "source": [
    "Now we choose genres to color, assign specific colors to them, and label the list of points we're calling \"outliers.\"\n",
    "\n",
    "If you want to change the colors, here's [a list of color names in python.](https://matplotlib.org/stable/gallery/color/named_colors.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-concern",
   "metadata": {},
   "outputs": [],
   "source": [
    "colorseq = []\n",
    "\n",
    "orangegenre = 'action'\n",
    "bluegenre = 'romance'\n",
    "\n",
    "for idx, row in moviesubset.iterrows():\n",
    "    thiscolor = genre2color(row, orangegenre, bluegenre)\n",
    "    colorseq.append(thiscolor)\n",
    "\n",
    "plotframe = pd.DataFrame({'x': components[ : , 0], 'y': components[ : , 1],\n",
    "                         'color': colorseq})\n",
    "colordict = {'other': 'whitesmoke', orangegenre: 'orange',\n",
    "            'both': 'greenyellow', bluegenre: 'deepskyblue'}\n",
    "\n",
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "theplot = sns.scatterplot(data = plotframe, x = 'x', y = 'y', hue = 'color',\n",
    "               palette = colordict)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "outliers = get_outliers(plotframe)  # this just gets a list of\n",
    "\n",
    "for line in outliers:\n",
    "     theplot.text(plotframe.x[line]+0.0005, plotframe.y[line] + .0005, \n",
    "     moviesubset.index[line], horizontalalignment='left', \n",
    "     size='medium', color='black', weight='normal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-outdoors",
   "metadata": {},
   "source": [
    "There could be some random variation between our plots. But I suspect many of us will see *Star Trek* movies on the right-hand side of this--and most of the romances in the lower left corner.\n",
    "\n",
    "This is *unsupervised*; the analysis doesn't know which movies belong to which human-perceived genre. It's just looking for a viewing angle that makes things spread out as much as possible. Since action movies are pretty different from romances, that happens to be an angle of view that makes those two genres relatively distinct.\n",
    "\n",
    "#### EXERCISE 2: EXPLORATION\n",
    "\n",
    "Try the same thing with different pairs of genres--\"comedy\" and \"romance,\" for instance, or \"sci-fi\" and \"horror\"--to see whether you get different patterns. You can also go back to the cell where we \"selected a set of movies,\" and change the random seed or the number of movies, to see if that changes the result.\n",
    "\n",
    "After 5 minutes of self-directed exploration, we'll convene in small groups, and each group will select a result to show.\n",
    "\n",
    "#### Understanding the axes of the plot\n",
    "\n",
    "If you want to understand the axes of PCA it *is* possible to extract an explanation. You can create a matrix ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "related-husband",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.066113</td>\n",
       "      <td>0.027135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>-0.017624</td>\n",
       "      <td>0.022736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>-0.050673</td>\n",
       "      <td>-0.025748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.031369</td>\n",
       "      <td>-0.016038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          PC1       PC2\n",
       "the  0.066113  0.027135\n",
       "it  -0.017624  0.022736\n",
       "you -0.050673 -0.025748\n",
       "to   0.031369 -0.016038"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadings = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2'], index=normed_subset.columns)\n",
    "loadings.iloc[0:4, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-madison",
   "metadata": {},
   "source": [
    "... and then sort each column to figure out which of the original variables play a big role in defining the ends of the axis. E.g., here's the right-hand (positive) side of the X axis. In many of the plots I've produced, this is a pole that attracts a lot of science fiction, and especially Star Trek movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings['PC1'].sort_values()[-20: ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-trance",
   "metadata": {},
   "source": [
    "The cell below is the lower (negative) side of the Y axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings['PC2'].sort_values()[0: 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-entity",
   "metadata": {},
   "source": [
    "#### If we have time: what's the effect of release date?\n",
    "\n",
    "Language changes pretty dramatically over time. Maybe our PCA space is being shaped not just by genre but by release date. We have this in the ```year``` column of the moviesubset dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "black-observer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='year', ylabel='Count'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAATCUlEQVR4nO3df5BdZ33f8ffHko3BxsGClarIUgVTxcGTiW1YDBhCEhQSk2SQSDA2+aUWt3Kn+QFNQiqa/hGaaesymYS0ySTWAM2mocTmh2vRpiaqYqCpXdtrYxwb2RFQYytWpcXA2A4TjM23f9yj8Xq1kq5We+5d7fN+zdw55zz3Pnu/+4z9uUfPnvPcVBWSpHacNu4CJEmjZfBLUmMMfklqjMEvSY0x+CWpMSvHXcAwXvSiF9XGjRvHXYYknVLuvPPOr1TVxNz2UyL4N27cyPT09LjLkKRTSpIvz9fuVI8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Je0bK1bv4EkC36sW79h3L9CL06JJRskaSEe2f8wV1x7y4L7X3f1pYtYzdLhGb8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqTG/Bn+T8JHfPejyW5J1JViXZnWRftz23rxokSUfqLfir6oGquqiqLgJeDnwDuAHYAeypqk3Anu5YkjQio5rq2Qx8saq+DGwBprr2KWDriGqQJDG64L8S+HC3v6aqDgB029XzdUiyPcl0kumZmZkRlSlJy1/vwZ/kDOBNwEdOpF9V7ayqyaqanJiY6Kc4SWrQKM743wjcVVUHu+ODSdYCdNtDI6hBktQZRfC/jWemeQB2Adu6/W3AjSOoQZLU6TX4kzwPeAPw8VnN1wBvSLKve+6aPmuQJD1br8syV9U3gBfOaXuUwVU+kqQx8M5dSWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTF9f9n6C5J8NMn9SfYmeXWSVUl2J9nXbc/tswZJ0rP1fcb/u8BNVfXdwIXAXmAHsKeqNgF7umNJ0oj0FvxJzgFeB3wAoKqerKqvA1uAqe5lU8DWvmqQJB2pzzP+lwAzwH9K8tkk709yFrCmqg4AdNvV83VOsj3JdJLpmZmZHsuUpLb0GfwrgZcBf1BVFwN/ywlM61TVzqqarKrJiYmJvmqUpOb0Gfz7gf1VdVt3/FEGHwQHk6wF6LaHeqxBkjRHb8FfVf8PeDjJ+V3TZuDzwC5gW9e2DbixrxokSUda2fPP/0XgQ0nOAL4E/CMGHzbXJ7kKeAi4vOcaJEmz9Br8VXU3MDnPU5v7fF9J0tF5564kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1/SkrVu/QaSLPih+fW9OqckLdgj+x/mimtvWXD/666+dBGrWT4845ekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mN6fVyziQPAo8DTwNPVdVkklXAdcBG4EHgrVX1tT7rkCQ9YxRn/D9YVRdV1eEvXd8B7KmqTcCe7liSNCLjmOrZAkx1+1PA1jHUIEnN6jv4C/jzJHcm2d61ramqAwDddvV8HZNsTzKdZHpmZqbnMiWpHX0v2fCaqnokyWpgd5L7h+1YVTuBnQCTk5PVV4GS1Jpez/ir6pFuewi4AbgEOJhkLUC3PdRnDZKkZ+st+JOcleT5h/eBHwbuBXYB27qXbQNu7KsGSdKR+pzqWQPc0C2NuhL4L1V1U5I7gOuTXAU8BFzeYw2SpDl6C/6q+hJw4TztjwKb+3pfSdKxeeeuJDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINfkhpj8EtSY4YK/iSvGaZNkrT0DXvG/x+HbJMkLXHHXI8/yauBS4GJJL8866lzgBV9FiZJ6sfxvojlDODs7nXPn9X+GPCWvoqSJPXnmMFfVZ8GPp3kj6rqyyOqSZLUo2G/evE5SXYCG2f3qarX91GUJKk/wwb/R4A/BN4PPH0ib5BkBTAN/E1V/XiSVcB1DD5EHgTeWlVfO5GfKUlauGGv6nmqqv6gqm6vqjsPP4bs+w5g76zjHcCeqtoE7OmOJUkjMmzwfyLJP0uyNsmqw4/jdUpyHvBjDP6lcNgWYKrbnwK2nkjBkqSTM+xUz7Zu+65ZbQW85Dj93gf8Gs++ImhNVR0AqKoDSVbP1zHJdmA7wIYNG4YsU5IW0WkrSXJSP+I7z1vP3zz80CIVtDiGCv6qevGJ/uAkPw4cqqo7k/zAifavqp3AToDJyck60f6SdNK+/RRXXHvLSf2I666+dJGKWTxDBX+Sn5uvvar++BjdXgO8KcmPAmcC5yT5E+BgkrXd2f5a4NCJFi1JWrhh5/hfMevxfcBvAG86VoeqendVnVdVG4Ergb+oqp8BdvHM1NE24MYTL1uStFDDTvX84uzjJN8B/OcFvuc1wPVJrgIeAi5f4M+RJC3AsH/cnesbwKZhX1xVnwI+1e0/Cmxe4PtKkk7SsHP8n2BwFQ8MFmd7KXB9X0VJkvoz7Bn/b83afwr4clXt76EeSVLPhvrjbrdY2/0Mrsc/F3iyz6IkSf0Z9hu43grczuAPsW8FbkvissySdAoadqrn14FXVNUhgCQTwP8EPtpXYZKkfgx7Hf9ph0O/8+gJ9JUkLSHDnvHflOSTwIe74yuAP+unJElSn473nbv/gMGiau9K8hPAa4EAtwIfGkF9kqRFdrzpmvcBjwNU1cer6per6p8zONt/X7+lSZL6cLzg31hV98xtrKppBt+gJUk6xRwv+M88xnPPXcxCJEmjcbzgvyPJP5nb2C2wNuxXL0qSlpDjXdXzTuCGJD/NM0E/CZwBvLnHuiRJPTlm8FfVQeDSJD8IfE/X/N+r6i96r0yS1Ith1+O/Gbi551okSSPg3beS1BiDX1Jv1q3fQJIFP9SPhX4DlyQd1yP7H+aKa29ZcP/rrr50EavRYZ7xS1Jjegv+JGcmuT3J55Lcl+Q9XfuqJLuT7Ou25/ZVgyTpSH2e8X8TeH1VXQhcBFyW5FXADmBPVW0C9nTHkqQR6S34a+CJ7vD07lHAFmCqa58CtvZVgyTpSL3O8SdZkeRu4BCwu6puY7DM8wGAbrv6KH23J5lOMj0zM9NnmZLUlF6Dv6qerqqLgPOAS5J8z3G6zO67s6omq2pyYmKitxolqTUjuaqnqr4OfAq4DDiYZC1Atz109J6SpMXW51U9E0le0O0/F/gh4H5gF7Cte9k24Ma+apB0crwBa3nq8wautcBUkhUMPmCur6r/luRW4PpuaeeHgMt7rEHSSfAGrOWpt+Dvvrnr4nnaHwU29/W+kqRj885dSWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTG9BX+S9UluTrI3yX1J3tG1r0qyO8m+bntuXzVIko7U5xn/U8CvVNVLgVcBP5/kAmAHsKeqNgF7umNJ0oj0FvxVdaCq7ur2Hwf2AuuALcBU97IpYGtfNUiSjjSSOf4kG4GLgduANVV1AAYfDsDqo/TZnmQ6yfTMzMwoypSkJvQe/EnOBj4GvLOqHhu2X1XtrKrJqpqcmJjor0BJakyvwZ/kdAah/6Gq+njXfDDJ2u75tcChPmuQJD1bn1f1BPgAsLeqfnvWU7uAbd3+NuDGvmqQJB1pZY8/+zXAzwJ/leTuru1fAtcA1ye5CngIuLzHGiRJc/QW/FX1l0CO8vTmvt5XknRs3rkrSY0x+CWpMQa/tIytW7+BJAt+aHnq84+7ksbskf0Pc8W1tyy4/3VXX7qI1Wip8Ixfkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1KfTVp7UTXTr1m9Y9JK8gUuS+vTtp5bcTXSe8UtLmEsuqA+e8UtLmEsuqA+e8UtSYwx+SWqMwS9JjTH4JakxvQV/kg8mOZTk3lltq5LsTrKv257b1/tLkubX5xn/HwGXzWnbAeypqk3Anu5YkjRCvQV/VX0G+Oqc5i3AVLc/BWzt6/0lSfMb9Rz/mqo6ANBtVx/thUm2J5lOMj0zMzOyAiVpuVuyf9ytqp1VNVlVkxMTE+MuR5KWjVEH/8EkawG67aERv78kNW/Uwb8L2NbtbwNuHPH7S1Lz+ryc88PArcD5SfYnuQq4BnhDkn3AG7pjSdII9bZIW1W97ShPbe7rPaXFtm79Bh7Z//CC+684/Tk8/a1vLmJF0slzdU7pGBZjdUxX19RSs2Sv6pEk9cPgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY0x+NWrdes3kGTBj3XrN4z7V5CWHW/gUq8W4wYoSYvLM35JaozBfxxOVYzZaSsdf2mROdVzHE5VjNm3n3L8pUW27IP/ZFdXPGndGetCnezqjuPuP3YnOf7ScrTsg3/sZ+yLcMZ6qvcfK//FIB3BOX5JaozBL0mNMfglqTEGvyQ1ZizBn+SyJA8k+UKSHeOoQZJaNfLgT7IC+H3gjcAFwNuSXDDqOiSpVeM4478E+EJVfamqngT+FNgyhjokqUmpqtG+YfIW4LKq+sfd8c8Cr6yqX5jzuu3A9u7wfOCBkRa6dLwI+Mq4izhFOFbDcZyGsxzG6e9X1cTcxnHcwDXfbZRHfPpU1U5gZ//lLG1Jpqtqctx1nAocq+E4TsNZzuM0jqme/cD6WcfnAY+MoQ5JatI4gv8OYFOSFyc5A7gS2DWGOiSpSSOf6qmqp5L8AvBJYAXwwaq6b9R1nEKan+46AY7VcByn4SzbcRr5H3clSePlnbuS1BiDX5IaY/CPQZIPJjmU5N5ZbRcmuTXJXyX5RJJz5vTZkOSJJL86q+3l3eu/kOQ/ZJl948iJjlOS7+2eu697/syu3XHqxinJ6Ummuva9Sd49q89yH6f1SW7ufu/7kryja1+VZHeSfd323Fl93t2NxwNJfmRW+6k9VlXlY8QP4HXAy4B7Z7XdAXx/t/924Dfn9PkY8BHgV2e13Q68msG9Ef8DeOO4f7dxjRODCxXuAS7sjl8IrHCcjhinnwL+tNt/HvAgsLGRcVoLvKzbfz7w1wyWjXkvsKNr3wH8+27/AuBzwHOAFwNfXC7/TXnGPwZV9Rngq3Oazwc+0+3vBn7y8BNJtgJfAu6b1bYWOKeqbq3Bf4l/DGztr+rRO8Fx+mHgnqr6XNf30ap62nECnj1OBZyVZCXwXOBJ4LFGxulAVd3V7T8O7AXWMVgyZqp72RTP/N5bGHxIfrOq/i/wBeCS5TBWBv/ScS/wpm7/crqb3JKcBfwL4D1zXr+Owc1wh+3v2pa7eccJ+C6gknwyyV1Jfq1rd5yePU4fBf4WOAA8BPxWVX2VxsYpyUbgYuA2YE1VHYDBhwOwunvZOmD2F3YfHpNTfqwM/qXj7cDPJ7mTwT9Dn+za3wP8TlU9Mef1Qy19sQwdbZxWAq8FfrrbvjnJZhynueN0CfA08J0Mpi9+JclLaGickpzNYOr0nVX12LFeOk9bHaP9lLHsv2z9VFFV9zOYriDJdwE/1j31SuAtSd4LvAD4dpK/Y/Af7nmzfkQTS18cY5z2A5+uqq90z/0Zg3nvP8Fxmj1OPwXcVFXfAg4l+d/AJPC/aGCckpzO4P+dD1XVx7vmg0nWVtWBbhrnUNd+tOVl9nOKj5Vn/EtEktXd9jTgXwF/CFBV31dVG6tqI/A+4N9W1e91/yR9PMmruisKfg64cSzFj9DRxonBneDfm+R53fz19wOfd5yOGKeHgNdn4CzgVcD9LYxT93t9ANhbVb8966ldwLZufxvP/N67gCuTPCfJi4FNwO3LYqzG/dflFh/AhxnMsX6LwdnDVcA7GFxl8NfANXR3Vc/p9xs8+6qeSQZzuV8Efm++Pqfy40THCfgZBn8Avxd4r+N05DgBZzO4Ouw+4PPAuxoap9cymJK5B7i7e/wogyvA9gD7uu2qWX1+vRuPB5h15c6pPlYu2SBJjXGqR5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4JdGJMmKcdcggcEvzSvJbx5er707/jdJfinJu5LckeSeJO+Z9fx/TXJnt8779lntTyT510luY7CMrzR2Br80vw/Q3cbfLXtwJXCQwW37lwAXAS9P8rru9W+vqpczuKPzl5K8sGs/i8E6+a+sqr8cYf3SUblImzSPqnowyaNJLgbWAJ8FXsFg4bPPdi87m8EHwWcYhP2bu/b1XfujDFbC/Ngoa5eOx+CXju79wD8E/h7wQWAz8O+q6trZL0ryA8APAa+uqm8k+RRwZvf031XV0yOqVxqKUz3S0d0AXMbgTP+T3ePt3XruJFnXrYL5HcDXutD/bgYrXkpLlmf80lFU1ZNJbga+3p21/3mSlwK3dt+t/QSDFUFvAv5pknsYrOL4f8ZVszQMV+eUjqL7o+5dwOVVtW/c9UiLxakeaR5JLmDw5dp7DH0tN57xS1JjPOOXpMYY/JLUGINfkhpj8EtSYwx+SWrM/wcyPZpytErzhAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(moviesubset.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have time: produce a visualization of movies in PCA space, where the\n",
    "# points are colored by release date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-marine",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "We've been using human-assigned labels to identify clusters of works, and then visualizing them in an abstract space that we inferred algorithmically using PCA.\n",
    "\n",
    "In *theory* we could also use an algorithm to infer discrete clusters of works, instead of relying on human-assigned genre labels.\n",
    "\n",
    "But I have to say, clustering algorithms are more a thing that gets *taught* in data science courses rather than a thing that works well, in practice, on real textual data. If your data is relatively simple and has sharp boundaries between species of iris, or species of penguin, clustering may work on it. Actual cultural data tends to have blurry boundaries and lots of dimensions, and clustering is not especially powerful in that situation.\n",
    "\n",
    "For an overview of different clustering algorithms available in scikit-learn you can [consult the documentation.](https://scikit-learn.org/stable/modules/clustering.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "piano-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-sculpture",
   "metadata": {},
   "source": [
    "In terms of programming, clustering is very simple. We create a clustering instance, and then fit it to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "dried-programmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, random_state=10).fit(normed_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(kmeans.labels_).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-reservoir",
   "metadata": {},
   "source": [
    "#### EXERCISE 3:\n",
    "\n",
    "Produce a visualization of movies in PCA space, with the points colored by kmeans cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "public-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
